{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a93f3d2",
   "metadata": {},
   "source": [
    "We'll cover the following topics:\n",
    "* Lexicons\n",
    "* Phonemes, graphemes, and morphemes\n",
    "* Tokenization\n",
    "* Understanding word normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb85218",
   "metadata": {},
   "source": [
    "## Lexicons- \n",
    "a lexicon can be thought of as a dictionary of terms that are called lexemes.\n",
    "For instance, the terms used by medical practitioners can be thought of as a lexicon for their\n",
    "profession. As an example, when trying to build an algorithm to convert a physical\n",
    "prescription provided by doctors into an electronic form, the lexicons would\n",
    "be primarily composed of medical terms.\n",
    "\n",
    "## Phonemes, graphemes, and morphemes\n",
    "* __Phonemes__ can be thought of as the speech sounds, made by the mouth or unit of\n",
    "sound, that can differentiate one word from another in a language.\n",
    "* __Graphemes__ are groups of letters of size one or more that can represent these\n",
    "individual sounds or phonemes. The word spoon consists of five letters that\n",
    "actually represent four phonemes, identified by the graphemes s, p, oo, and n.\n",
    "* A __morpheme__ is the smallest meaningful unit in a language. The word unbreakable\n",
    "is composed of three morphemes:\n",
    "    * un—a bound morpheme signifying not\n",
    "    * break—the root morpheme\n",
    "    * able—a free morpheme signifying can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b9ad6",
   "metadata": {},
   "source": [
    "## __Tokenization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51cb467",
   "metadata": {},
   "source": [
    "## Blankline Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1927d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.',\n",
       " 'I want a book as well']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n\\n I want a book as well\"\n",
    "tokenizer = BlanklineTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918ba0c",
   "metadata": {},
   "source": [
    "## WordPunct Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877513b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'costs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " '$',\n",
       " '3000',\n",
       " '.',\n",
       " '0',\n",
       " '-',\n",
       " '$',\n",
       " '8000',\n",
       " '.',\n",
       " '0',\n",
       " 'in',\n",
       " 'USA',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'a',\n",
       " 'book',\n",
       " 'as',\n",
       " 'well']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n I want a book as well\"\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380a5f0",
   "metadata": {},
   "source": [
    "## Regular expressions-based tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb4eb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: \"\\w\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\w\"? A raw string is also an option.\n",
      "<>:3: SyntaxWarning: \"\\w\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\w\"? A raw string is also an option.\n",
      "/var/folders/mm/63k15_ws007cj6mjbyxwt_7m0000gp/T/ipykernel_61751/1566248286.py:3: SyntaxWarning: \"\\w\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\w\"? A raw string is also an option.\n",
      "  tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'costs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " '$3000.0',\n",
       " '-',\n",
       " '$8000.0',\n",
       " 'in',\n",
       " 'USA',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2cdd4",
   "metadata": {},
   "source": [
    "The `\\w+|\\$[\\d\\.]+|\\S+` regular expression allows three alternative patterns:\n",
    "* First alternative: \\w+ that matches any word character (equal to [a-zA-Z0-9_]).\n",
    "The + is a quantifier and matches between one and unlimited times as many\n",
    "times as possible.\n",
    "* Second alternative: \\$[\\d\\.]+. Here, \\$ matches the character $, \\d matches a\n",
    "digit between 0 and 9, \\. matches the character . (period), and + again acts as a\n",
    "quantifier matching between one and unlimited times.\n",
    "* Third alternative: \\S+. Here, \\S accepts any non-whitespace character and +\n",
    "again acts the same way as in the preceding two alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17adaf89",
   "metadata": {},
   "source": [
    "## Treebank tokenizer\n",
    "The Treebank tokenizer does a great job of splitting contractions such as doesn't to does and\n",
    "n't. It further identifies periods at the ends of lines and eliminates them. Punctuation such\n",
    "as commas is split if followed by whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766ff91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'cost',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '3000.0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80893f4",
   "metadata": {},
   "source": [
    "## TweetTokenizer\n",
    "\n",
    "The rise of social media has given rise to an informal language wherein\n",
    "people tag each other using their social media handles and use a lot of emoticons, hashtags,\n",
    "and abbreviated text to express themselves. We need tokenizers in place that can parse such\n",
    "text and make things more understandable. TweetTokenizer caters to this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e67303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@amankedia',\n",
       " \"i'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'rolexxxxxxxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxxxx Watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer(preserve_case=False) # by default preserve_case is kept as True\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625f1ac",
   "metadata": {},
   "source": [
    "## __Word Normalization__\n",
    "\n",
    "Intro - We can also bring words to their root form in the\n",
    "dictionary. For instance, am, are, and is can be identified by their root form, be. On another\n",
    "front, we can remove inflections from words to bring them down to the same form. Words\n",
    "car, cars, and car's can all be identified as car.\n",
    "\n",
    "Also, common words that occur very frequently and do not convey much meaning, such as\n",
    "the articles a, an, and the, can be removed. However, all these highly depend on the use\n",
    "cases. Wh- words, such as when, why, where, and who, do not carry much information in\n",
    "most contexts and are removed as part of a technique called stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd3c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
    "           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece4b05",
   "metadata": {},
   "source": [
    "## Stemming - \n",
    "Bringing all of the words computer, computerization, and computerize into one\n",
    "word, compute. What happens here is called stemming. As part of stemming, a crude\n",
    "attempt is made to remove the inflectional forms of a word and bring them to a base form\n",
    "called the stem. The chopped-off pieces are referred to as affixes.\n",
    "\n",
    "The two most common algorithms/methods employed for stemming include the \n",
    "__Porter stemmer__ and the __Snowball stemmer__. \n",
    "\n",
    "The Porter stemmer supports the English language,\n",
    "whereas the Snowball stemmer, which is an improvement on the Porter stemmer, supports\n",
    "multiple languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a0fe31",
   "metadata": {},
   "source": [
    "## Porter Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01fad2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8785e7",
   "metadata": {},
   "source": [
    "## Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a818170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73fc8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
     ]
    }
   ],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "singles = [stemmer2.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccba717",
   "metadata": {},
   "source": [
    "In most of cases, its output is similar to that of the\n",
    "Porter stemmer, except for generously, where the Porter stemmer outputs gener and the\n",
    "Snowball stemmer outputs generous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94580029",
   "metadata": {},
   "source": [
    "## __Lemmatization__\n",
    "\n",
    "Intro - lemmatization is a process wherein the context is used to convert a word to its meaningful\n",
    "base form. It helps in grouping together words that have a common base form and so can\n",
    "be identified as a single item.The base form is referred to as the lemma of the word and is\n",
    "also sometimes known as the dictionary form.\n",
    "\n",
    "\n",
    "A lemmatizer\n",
    "would try and identify the part-of-speech tags based on the context to identify the\n",
    "appropriate lemma. \\\n",
    "The most commonly used lemmatizer is the WordNet lemmatizer.\n",
    "Other lemmatizers include the Spacy lemmatizer, TextBlob lemmatizer, and Gensim\n",
    "lemmatizer, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3c7bd",
   "metadata": {},
   "source": [
    "## WordNet lemmatizerAs part of\n",
    "WordNet, nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive\n",
    "synonyms (synsets), each expressing distinct concepts. These synsets are interlinked using\n",
    "lexical and conceptual semantic relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd9536b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute nltk.download('wordnet') on terminal\n",
    "from nltk.stem import WordNetLemmatizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cacb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
      "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "token_list = s.split()\n",
    "print(\"The tokens are: \", token_list)\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
    "print(\"The lemmatized output is: \", lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56beab26",
   "metadata": {},
   "source": [
    "As can be seen, the WordNet lemmatizer did not do much here. Out of are, putting,\n",
    "efforts, and understanding, none were converted to their base form.\n",
    "\n",
    "The WordNet lemmatizer works well if the `POS tags` are also provided as inputs.\n",
    "It is really impossible to manually annotate each word with its POS tag in a text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc80ba",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bcabcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('putting', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('enhance', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('understanding', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Lemmatization', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(token_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0b78b",
   "metadata": {},
   "source": [
    "As can be seen, a list of tuples of the form (the token and POS tag) is returned by the POS\n",
    "tagger. Now, the POS tags need to be converted to a form that can be understood by the\n",
    "WordNet lemmatizer and sent in as input along with the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da416f2",
   "metadata": {},
   "source": [
    "## POS tag Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152b9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "##This is a common method which is widely used across the NLP community of practitioners\n",
    "\n",
    "def get_part_of_speech_tags(token):\n",
    "    \n",
    "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
    "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69f5af",
   "metadata": {},
   "source": [
    "## Wordnet Lemmatizer with POS Tag Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9da03b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We be put in effort to enhance our understand of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
    "print(' '.join(lemmatized_output_with_POS_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f3efb",
   "metadata": {},
   "source": [
    "The following conversions happened:\\\n",
    "are to __be__\\\n",
    "putting to __put__\\\n",
    "efforts to __effort__\\\n",
    "understanding to __understand__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9831fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are put in effort to enhanc our understand of lemmat\n"
     ]
    }
   ],
   "source": [
    "## Lets compare with snowball stemmer - \n",
    "stemmer2 = SnowballStemmer(language='english')\n",
    "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
    "print(' '.join(stemmed_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2ab85",
   "metadata": {},
   "source": [
    "the WordNet lemmatizer makes a sensible and context-aware conversion of\n",
    "the token into its base form, unlike the stemmer, which tries to chop the affixes from the\n",
    "token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aca640",
   "metadata": {},
   "source": [
    "## Spacy lemmatizer\n",
    "\n",
    "This comes up with pretrained models that can parse text and figure out the\n",
    "various properties of the text, such as POS tags, named-entity tags, and so on, with a simple\n",
    "function call. The prebuilt models identify the POS tags and assign a lemma to each token,\n",
    "unlike the WordNet lemmatizer, where the POS tags need to be explicitly provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336217e",
   "metadata": {},
   "source": [
    "`pip install spacy && python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6a7b3",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Stopwords are words such as a, an, the, in, at, and so on that occur frequently in text corpora\n",
    "and do not carry a lot of information in most contexts. These words, in general, are required\n",
    "for the completion of sentences and making them grammatically sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034c445b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"above, few, didn't, all, we're, over, needn, by, they'd, re, then, aren, same, he'll, once, of, m, didn, wasn't, own, your, below, she'll, couldn, no, it's, doing, d, me, be, ours, until, at, should've, isn't, wasn, do, we'd, whom, you're, further, it'd, have, was, on, other, shan, they're, so, before, yourself, hasn, with, it, you've, t, him, any, this, and, both, those, again, yourselves, s, hers, but, out, most, myself, his, too, being, into, or, for, very, just, mustn't, wouldn't, we, than, ain, such, yours, its, them, she, hasn't, he, weren, after, i've, if, itself, wouldn, her, are, should, he's, mightn, there, ve, couldn't, don, herself, that, won't, had, you'll, won, been, is, it'll, now, ma, their, about, did, does, my, needn't, they've, she's, against, how, during, up, some, you, not, i, she'd, themselves, weren't, o, ourselves, a, shan't, mightn't, we've, has, ll, these, hadn, only, while, i'm, where, in, having, i'd, our, himself, mustn, each, doesn't, haven, haven't, that'll, am, here, they, why, down, they'll, doesn, theirs, which, who, because, will, i'll, as, hadn't, nor, through, when, y, shouldn, shouldn't, isn, you'd, don't, an, from, aren't, the, to, were, off, under, what, more, he'd, can, we'll, between\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\", \".join(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37040eb8",
   "metadata": {},
   "source": [
    "If you look closely, you'll notice that Wh- words such as who, what, when, why, how, which,\n",
    "where, and whom are part of this list of stopwords; however, in one of the previous sections,\n",
    "it was mentioned that these words are very significant in use cases such as question\n",
    "answering and question classification. Measures should be taken to ensure that these words\n",
    "are not filtered out when the text corpus undergoes stopword removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083dec41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how putting efforts enhance understanding Lemmatization'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
    "\n",
    "## Eliminating the required Wh words from our stop words to make our collection healthy for checking question/answer context\n",
    "for word in wh_words:\n",
    "    stop.remove(word)\n",
    "\n",
    "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
    "\" \".join(sentence_after_stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd06ff5",
   "metadata": {},
   "source": [
    "The stopwords are, we, in, to,\n",
    "our, and of were removed from the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec5289",
   "metadata": {},
   "source": [
    "## Case Folding\n",
    "As part of case\n",
    "folding, all the letters in the text corpus are converted to lowercase. The and the will be\n",
    "treated the same in a scenario of case folding,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6ea64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are putting in efforts to enhance our understanding of lemmatization'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed3df3",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Until now, we have focused on tokens of size 1, which means only one word. Sentences\n",
    "generally contain names of people and places and other open compound terms, such as\n",
    "living room and coffee mug. These phrases convey a specific meaning when two or more\n",
    "words are used together. When used individually, they carry a different meaning\n",
    "altogether and the inherent meaning behind the compound terms is somewhat lost. The\n",
    "usage of multiple tokens to represent such inherent meaning can be highly beneficial for the\n",
    "NLP tasks being performed.\n",
    "\n",
    "When n is equal to 1,\n",
    "these are termed as unigrams. Bigrams, or 2-grams, refer to pairs of words, such as dinner\n",
    "table. Phrases such as the United Arab Emirates comprising three words are termed as\n",
    "trigrams or 3-grams. This naming system can be extended to larger n-grams, but most NLP\n",
    "tasks use only trigrams or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ceb71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language',\n",
       " 'Language Processing',\n",
       " 'Processing is',\n",
       " 'is the',\n",
       " 'the way',\n",
       " 'way to',\n",
       " 'to go']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bi-grams\n",
    "from nltk.util import ngrams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "[\" \".join(token) for token in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79443f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing',\n",
       " 'Language Processing is',\n",
       " 'Processing is the',\n",
       " 'is the way',\n",
       " 'the way to',\n",
       " 'way to go']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tri-grams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "[\" \".join(token) for token in trigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dfe62",
   "metadata": {},
   "source": [
    "## Building a basic vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a5a251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = set(s.split())\n",
    "vocabulary = sorted(tokens)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383872a",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ebdd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My First HeadingMy first paragraph.\n"
     ]
    }
   ],
   "source": [
    "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
